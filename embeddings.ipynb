{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 — Embeddings and Tokenization\n",
    "\n",
    "In this notebook I reproduce and experiment with the core ideas from Chapter 2 of *Build a Large Language Model (From Scratch)* by Sebastian Raschka.\n",
    "\n",
    "This chapter is fundamental because it explains how raw text is transformed into numerical representations (embeddings), which are the foundation of Large Language Models (LLMs).\n",
    "\n",
    "LLMs do not understand words directly — they operate on vectors. Therefore, understanding tokenization and embeddings is critical for understanding how transformers, attention mechanisms, and agentic systems work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SANTIAGO AMAYA\\TDSE\\TDSE_LLM_Text_Preprocessing_Foundations_Embeddings\\.venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:283: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(raw_text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Tokenization Matters\n",
    "\n",
    "Neural networks cannot process raw text. They require numerical inputs.\n",
    "\n",
    "Tokenization converts text into discrete units (tokens). These tokens can represent:\n",
    "- Words\n",
    "- Subwords\n",
    "- Characters\n",
    "- Byte pairs (BPE)\n",
    "\n",
    "Modern LLMs use subword tokenization (like BPE) because:\n",
    "- It reduces vocabulary size\n",
    "- It handles unknown words\n",
    "- It captures morphological structure\n",
    "\n",
    "Tokenization is the bridge between language and neural computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 5145\n",
      "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "encoded_text = tokenizer.encode(raw_text)\n",
    "\n",
    "print(\"Total tokens:\", len(encoded_text))\n",
    "print(encoded_text[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow,\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(encoded_text[:50])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Embeddings Encode Meaning\n",
    "\n",
    "An embedding is a dense vector representation of a token.\n",
    "\n",
    "Instead of representing words as one-hot vectors (which are sparse and meaningless geometrically),\n",
    "embeddings map each token to a dense vector in continuous space.\n",
    "\n",
    "Why does this encode meaning?\n",
    "\n",
    "Because during training:\n",
    "- Tokens that appear in similar contexts receive similar gradient updates.\n",
    "- The neural network adjusts their vector positions to minimize prediction error.\n",
    "\n",
    "As a result:\n",
    "- Words with similar meanings end up near each other in vector space.\n",
    "- Relationships become linear directions in embedding space.\n",
    "\n",
    "This is deeply related to core neural network concepts:\n",
    "- Parameters (weights) are learned via backpropagation\n",
    "- Embeddings are simply trainable weight matrices\n",
    "- Each row of the embedding matrix corresponds to a token vector\n",
    "\n",
    "So embeddings are not magic — they are learned parameters optimized to improve next-token prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 5141\n",
      "Example input: [40, 367, 2885, 1464]\n",
      "Example target: [367, 2885, 1464, 1807]\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "stride = 1\n",
    "\n",
    "input_ids = []\n",
    "target_ids = []\n",
    "\n",
    "for i in range(0, len(encoded_text) - max_length, stride):\n",
    "    input_chunk = encoded_text[i:i+max_length]\n",
    "    target_chunk = encoded_text[i+1:i+max_length+1]\n",
    "    \n",
    "    input_ids.append(input_chunk)\n",
    "    target_ids.append(target_chunk)\n",
    "\n",
    "print(\"Number of samples:\", len(input_ids))\n",
    "print(\"Example input:\", input_ids[0])\n",
    "print(\"Example target:\", target_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Sliding Windows Matter for LLMs\n",
    "\n",
    "LLMs are trained to predict the next token given previous tokens.\n",
    "\n",
    "To do this, we create training samples using sliding windows:\n",
    "- Input: sequence of tokens\n",
    "- Target: same sequence shifted by 1 position\n",
    "\n",
    "The stride controls how much we shift the window each time.\n",
    "\n",
    "Small stride → more overlap → more training samples  \n",
    "Large stride → fewer samples → less redundancy\n",
    "\n",
    "Overlap is useful because:\n",
    "- It increases training data size\n",
    "- It allows the model to see similar contexts with slight variations\n",
    "- It improves statistical learning stability\n",
    "\n",
    "This is especially important for small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5141, 4])\n",
      "torch.Size([5141, 4])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor(input_ids)\n",
    "targets = torch.tensor(target_ids)\n",
    "\n",
    "print(inputs.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding output shape: torch.Size([5141, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "embedding_dim = 256\n",
    "\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "embedded = embedding_layer(inputs)\n",
    "\n",
    "print(\"Embedding output shape:\", embedded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Embeddings Are Critical for Agentic Systems\n",
    "\n",
    "Agentic systems rely on:\n",
    "- Memory\n",
    "- Retrieval (RAG)\n",
    "- Semantic similarity\n",
    "- Planning via vector comparisons\n",
    "\n",
    "All of these depend on embeddings.\n",
    "\n",
    "For example:\n",
    "- Vector databases store embeddings\n",
    "- Similarity search is done via cosine similarity\n",
    "- Agents retrieve relevant context based on vector distance\n",
    "\n",
    "Without embeddings:\n",
    "- No semantic search\n",
    "- No contextual memory\n",
    "- No reasoning over meaning\n",
    "\n",
    "Embeddings transform language into geometry.\n",
    "And geometry is computable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length=4, stride=1: 5141\n",
      "max_length=4, stride=2: 2571\n",
      "max_length=8, stride=1: 5137\n",
      "max_length=8, stride=4: 1285\n"
     ]
    }
   ],
   "source": [
    "def count_samples(max_length, stride):\n",
    "    count = 0\n",
    "    for i in range(0, len(encoded_text) - max_length, stride):\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "print(\"max_length=4, stride=1:\", count_samples(4,1))\n",
    "print(\"max_length=4, stride=2:\", count_samples(4,2))\n",
    "print(\"max_length=8, stride=1:\", count_samples(8,1))\n",
    "print(\"max_length=8, stride=4:\", count_samples(8,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Results and Interpretation\n",
    "\n",
    "When stride is small:\n",
    "- The number of samples increases dramatically.\n",
    "- Windows overlap heavily.\n",
    "- The model sees similar contexts multiple times.\n",
    "\n",
    "When stride increases:\n",
    "- The number of samples decreases.\n",
    "- Less redundancy.\n",
    "- Faster dataset creation but potentially less learning signal.\n",
    "\n",
    "Changing max_length:\n",
    "- Larger max_length reduces number of samples.\n",
    "- But increases context per sample.\n",
    "\n",
    "Why overlap is useful:\n",
    "\n",
    "Overlap helps the model:\n",
    "- Learn smoother transitions between tokens\n",
    "- Generalize better\n",
    "- Improve next-token probability estimation\n",
    "\n",
    "In real LLM training:\n",
    "- Overlapping windows are essential\n",
    "- They increase effective dataset size\n",
    "- They stabilize gradient updates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
